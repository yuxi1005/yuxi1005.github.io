<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>关于Transformer</title>
  
  <link rel="canonical" href="https://yuxi1005.github.io/2025/02/24/transformer/">
  
  <meta name="description" content="总述Transformer 全面介绍 一、背景与意义一个文本变换模型：序列到序列 Transformer 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过自注意力机制（Self-Attention）完全替代了传统的循环神经网络（RNN）和">
  
  
  <meta name="author" content="Yuxi">
  
  
  
  <meta property="og:site_name" content="yuxi" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="关于Transformer" />
  
  <meta property="og:description" content="总述Transformer 全面介绍 一、背景与意义一个文本变换模型：序列到序列 Transformer 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过自注意力机制（Self-Attention）完全替代了传统的循环神经网络（RNN）和">
  
  <meta property="og:url" content="https://yuxi1005.github.io/2025/02/24/transformer/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="关于Transformer">
  
  <meta name="twitter:description" content="总述Transformer 全面介绍 一、背景与意义一个文本变换模型：序列到序列 Transformer 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过自注意力机制（Self-Attention）完全替代了传统的循环神经网络（RNN）和">
  
  
  
  
  <meta name="twitter:url" content="https://yuxi1005.github.io/2025/02/24/transformer/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

  
  <script src="/js/pic.min.js" defer></script>
  

  

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">🌑</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>☀️</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hi Folks.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/Works" class="ml">Works</a>
          
        
          
          <a href="/About" class="ml">About</a>
          
        
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>关于Transformer</h2>

  <h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><h3 id="Transformer-全面介绍"><a href="#Transformer-全面介绍" class="headerlink" title="Transformer 全面介绍"></a>Transformer 全面介绍</h3><hr>
<h4 id="一、背景与意义"><a href="#一、背景与意义" class="headerlink" title="一、背景与意义"></a>一、背景与意义</h4><p>一个文本变换模型：序列到序列</p>
<p><strong>Transformer</strong> 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过<strong>自注意力机制（Self-Attention）</strong>完全替代了传统的循环神经网络（RNN）和卷积神经网络（CNN），解决了以下核心问题：  </p>
<ol>
<li><strong>长距离依赖</strong>：RNN难以捕捉长序列中的远距离关联。  </li>
<li><strong>并行计算</strong>：Transformer无需按序列顺序处理数据，可并行计算，大幅提升训练效率。  </li>
<li><strong>模型泛化</strong>：通过注意力机制动态学习不同位置的权重，适应多样化的上下文关系。</li>
</ol>
<span id="more"></span>

<hr>
<h4 id="二、核心架构"><a href="#二、核心架构" class="headerlink" title="二、核心架构"></a>二、核心架构</h4><p><img src="https://raw.githubusercontent.com/yuxi1005/yuxi1005.github.io/master/pictures/image-20250224160754290.png" alt="image-20250224160754290"></p>
<p>Transformer由<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>堆叠而成，每层结构独立且参数不共享。以下为关键组件：  </p>
<h5 id="1-编码器（Encoder）"><a href="#1-编码器（Encoder）" class="headerlink" title="1. 编码器（Encoder）"></a>1. <strong>编码器（Encoder）</strong></h5><ul>
<li><strong>输入嵌入（Input Embedding）</strong>：将输入词转换为高维向量。  </li>
<li><strong>位置编码（Positional Encoding）</strong>：为词向量注入位置信息（因Transformer无时序处理能力）。  </li>
<li><strong>多头自注意力（Multi-Head Self-Attention）</strong>：捕捉词与词之间的全局依赖关系。  </li>
<li><strong>前馈网络（Feed-Forward Network）</strong>：通过全连接层进行非线性变换。  </li>
<li><strong>残差连接（Residual Connection）</strong>与<strong>层归一化（Layer Normalization）</strong>：缓解梯度消失，加速收敛。</li>
</ul>
<h5 id="2-解码器（Decoder）"><a href="#2-解码器（Decoder）" class="headerlink" title="2. 解码器（Decoder）"></a>2. <strong>解码器（Decoder）</strong></h5><ul>
<li><strong>掩码多头注意力（Masked Multi-Head Attention）</strong>：防止未来信息泄露（训练时仅关注当前位置之前的词）。  </li>
<li><strong>编码器-解码器注意力（Encoder-Decoder Attention）</strong>：将编码器的输出作为Key和Value，解码器输入作为Query，实现跨序列对齐。  </li>
<li>其余结构与编码器类似（前馈网络、残差连接等）。</li>
</ul>
<hr>
<h4 id="三、自注意力机制（Self-Attention）"><a href="#三、自注意力机制（Self-Attention）" class="headerlink" title="三、自注意力机制（Self-Attention）"></a>三、自注意力机制（Self-Attention）</h4><p><strong>自注意力是Transformer的核心</strong>，通过计算词与词之间的关联权重，动态聚合上下文信息。其计算过程如下：  </p>
<ol>
<li><p><strong>生成Q、K、V矩阵</strong>：  </p>
<p>输入向量通过线性变换生成<strong>查询（Query）、键（Key）、值（Value）</strong>矩阵。  </p>
</li>
<li><p><strong>计算注意力分数</strong>：  </p>
<p>通过点积计算词与词之间的相似度，再缩放（防止梯度爆炸）并归一化：<br>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\<br>{d_k}为Key的维度<br>$$</p>
</li>
<li><p><strong>多头注意力（Multi-Head）</strong>：  </p>
<ul>
<li>将Q、K、V拆分为多个子空间（如8个“头”），分别计算注意力后拼接结果，增强模型捕捉不同层面信息的能力。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="四、位置编码（Positional-Encoding）"><a href="#四、位置编码（Positional-Encoding）" class="headerlink" title="四、位置编码（Positional Encoding）"></a>四、位置编码（Positional Encoding）</h4><p>因为transformer是并行计算每个词语的Attention的，所以没法考虑到语序问题。因此引入了位置编码。通过位置编码为输入序列注入位置信息。</p>
<ul>
<li><p><strong>正弦与余弦函数</strong>：<br>$$<br>PE_{(pos, 2i)} &#x3D; \sin\left(\frac{pos}{10000^{2i&#x2F;d_{\text{model}}}}\right), \quad<br>PE_{(pos, 2i+1)} &#x3D; \cos\left(\frac{pos}{10000^{2i&#x2F;d_{\text{model}}}}\right)<br>$$<br>(pos)：词的位置，(i)：维度索引。  </p>
<p>可泛化到任意长度的序列，且能通过线性变换捕捉相对位置关系。</p>
</li>
</ul>
<hr>
<h4 id="五、前馈网络（Feed-Forward-Network）"><a href="#五、前馈网络（Feed-Forward-Network）" class="headerlink" title="五、前馈网络（Feed-Forward Network）"></a>五、前馈网络（Feed-Forward Network）</h4><p>每个注意力层后接一个全连接前馈网络，包含两次线性变换和ReLU激活函数：<br>$$<br>\text{FFN}(x) &#x3D; \max(0, xW_1 + b_1)W_2 + b_2<br>$$</p>
<ul>
<li>作用：增强模型的非线性表达能力。</li>
</ul>
<hr>
<h4 id="六、训练与优化"><a href="#六、训练与优化" class="headerlink" title="六、训练与优化"></a>六、训练与优化</h4><ol>
<li><strong>损失函数</strong>：交叉熵损失（如机器翻译任务）。  </li>
<li><strong>优化器</strong>：Adam优化器，结合学习率预热（Warmup）和衰减策略。  </li>
<li><strong>正则化</strong>：  <ul>
<li>Dropout：应用于注意力权重和全连接层。  </li>
<li>标签平滑（Label Smoothing）：缓解过拟合。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="七、优势与局限性"><a href="#七、优势与局限性" class="headerlink" title="七、优势与局限性"></a>七、优势与局限性</h4><h5 id="优势："><a href="#优势：" class="headerlink" title="优势："></a>优势：</h5><ul>
<li><strong>并行计算</strong>：显著提升训练速度。  </li>
<li><strong>全局上下文建模</strong>：自注意力机制可捕捉任意距离的依赖关系。  </li>
<li><strong>可扩展性</strong>：模型深度和宽度灵活调整（如BERT、GPT等变体）。</li>
</ul>
<h5 id="局限性："><a href="#局限性：" class="headerlink" title="局限性："></a>局限性：</h5><ul>
<li><strong>计算复杂度</strong>：序列长度(n)的平方级复杂度（(O(n^2))），难以处理超长序列。  </li>
<li><strong>位置编码瓶颈</strong>：预设的位置编码可能无法完美适应所有任务。  </li>
<li><strong>显存消耗</strong>：多头注意力导致参数量较大。</li>
</ul>
<hr>
<h4 id="八、重要变体与改进"><a href="#八、重要变体与改进" class="headerlink" title="八、重要变体与改进"></a>八、重要变体与改进</h4><ol>
<li><strong>BERT</strong>：仅用编码器，通过掩码语言模型预训练。  </li>
<li><strong>GPT系列</strong>：仅用解码器，自回归生成文本。  </li>
<li><strong>Efficient Transformers</strong>：  <ul>
<li><strong>Longformer</strong>：稀疏注意力机制处理长文本。  </li>
<li><strong>Linformer</strong>：低秩近似降低计算复杂度。</li>
</ul>
</li>
<li><strong>视觉Transformer（ViT）</strong>：将图像分块输入Transformer。</li>
</ol>
<hr>
<h4 id="九、总结"><a href="#九、总结" class="headerlink" title="九、总结"></a>九、总结</h4><p>Transformer凭借其<strong>自注意力机制</strong>和<strong>并行化设计</strong>，成为深度学习领域的基石模型。它不仅推动了NLP的快速发展，还跨界影响了计算机视觉、语音识别等领域。尽管存在计算复杂度高、长序列处理难等挑战，其灵活性和强大性能使其成为AI模型设计的核心范式。后续的改进模型（如稀疏注意力、模型压缩等）进一步扩展了其应用边界，奠定了其在现代AI中的核心地位。</p>
<h2 id="关于前馈网络层"><a href="#关于前馈网络层" class="headerlink" title="关于前馈网络层"></a>关于前馈网络层</h2><p>前馈网络层，特别是在前馈神经网络中，扮演着至关重要的角色。前馈神经网络是一种最简单的神经网络，各神经元分层排列，每个神经元只与前一层的神经元相连，接收前一层的输出，并输出给下一层，各层间没有反馈。前馈网络层的作用：</p>
<ol>
<li><p><strong>特征提取与转换</strong></p>
<p>隐藏层（即前馈网络中的中间层）的主要功能是提取输入数据的特征和抽象表示。通过具有多个隐藏层，神经网络可以学习输入数据中越来越复杂和抽象的特征。隐藏层中的每个神经元都接收来自前一层神经元的输入，对其进行处理（如加权、求和、激活等），并将其传递到下一层。这样，隐藏层可以转换输入数据并提取有用的特征，从而使网络能够学习输入和输出之间更复杂和抽象的关系。</p>
</li>
<li><p><strong>非线性映射</strong>：</p>
<p>激活函数（如ReLU、sigmoid、tanh等）在隐藏层中的应用引入了非线性，使得神经网络能够学习和建模输入和输出之间更复杂的非线性关系。</p>
</li>
<li><p><strong>信息传递与整合</strong>：</p>
<p>在前馈神经网络中，信息从输入层流向隐藏层，再流向输出层，而不会循环回馈。这种单向信息传递的方式使得网络结构清晰、易于理解和实现。同时，每一层都接收来自前一层的信息，并对其进行整合和处理，最终生成网络的输出。</p>
</li>
</ol>

  <p><a class="classtest-link" href="/tags/transformer/" rel="tag">transformer</a>, <a class="classtest-link" href="/tags/%E8%AF%BB%E8%AE%BA%E6%96%87/" rel="tag">读论文</a> — Feb 24, 2025</p>
  


        </div>
        <div class="row mt-2">
  <h3>Search</h3>
  <div><input id="search-text" title="search" class="search-text" type="text" placeholder="search......"></div>
  <div style="margin-top: 1.5rem;">
    <ul id="result"></ul>
  </div>
</div>
        <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ❤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
        at <a href="https://en.wikipedia.org/wiki/Earth" target="_blank" rel="noreferrer">Earth</a>.</p>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://linkedin.com/in/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="LinkedIn">
        <svg class="linkedin svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://twitter.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Twitter">
        <svg class="twitter svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter</title><path d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://instagram.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Instagram">
        <svg class="instagram svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://stackoverflow.com/story/tobiasreithmeier" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="StackOverflow">
        <svg class="stackoverflow svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Stack Overflow</title><path d="M15.725 0l-1.72 1.277 6.39 8.588 1.716-1.277L15.725 0zm-3.94 3.418l-1.369 1.644 8.225 6.85 1.369-1.644-8.225-6.85zm-3.15 4.465l-.905 1.94 9.702 4.517.904-1.94-9.701-4.517zm-1.85 4.86l-.44 2.093 10.473 2.201.44-2.092-10.473-2.203zM1.89 15.47V24h19.19v-8.53h-2.133v6.397H4.021v-6.396H1.89zm4.265 2.133v2.13h10.66v-2.13H6.154Z"/></svg>
      </a>
      

    </div>
  
</div>

      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>

  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>