<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <!--Description-->

    

    
        <meta name="description" content="我的小窝"/>
    

    <!--Author-->
    
        <meta name="author" content="Yuxi"/>
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="自回归建模和双向上下文建模BERT"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="我的小窝"/>
    

    <!--Open Graph Site Name-->
        <meta property="og:site_name" content="yuxi"/>

    <!--Type page-->
    
        <meta property="og:type" content="article"/>
    

    <!--Page Cover-->
    
    
        <meta property="og:image" content="https://yuxi1005.github.ioimg/ocean.jpg"/>
    

        <meta name="twitter:card" content="summary_large_image"/>

    

    
        <meta name="twitter:image" content="https://yuxi1005.github.ioimg/ocean.jpg"/>
    

    <!-- Title -->
    
    <title>自回归建模和双向上下文建模BERT - yuxi</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"/>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet"/>

    <!-- Google Analytics -->
    


    <!-- favicon -->
    

<meta name="generator" content="Hexo 7.3.0"></head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Configurable Title</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a target="_blank" rel="noopener" href="https://github.com/klugjo/hexo-theme-clean-blog">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/ocean.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>自回归建模和双向上下文建模BERT</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2025-03-04
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/必备知识/">#必备知识</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <h1 id="Autoregressive-Language-Modeling-Masked-Language-Modeling-MLM"><a href="#Autoregressive-Language-Modeling-Masked-Language-Modeling-MLM" class="headerlink" title="Autoregressive Language Modeling &amp;&amp;  Masked Language Modeling, MLM"></a>Autoregressive Language Modeling &amp;&amp;  Masked Language Modeling, MLM</h1><hr>
<ul>
<li><h3 id="1-自回归语言建模（Autoregressive-Language-Modeling）"><a href="#1-自回归语言建模（Autoregressive-Language-Modeling）" class="headerlink" title="1. 自回归语言建模（Autoregressive Language Modeling）"></a>1. <strong>自回归语言建模（Autoregressive Language Modeling）</strong></h3><p>自回归语言建模是一种从左到右（或从右到左）逐词生成文本的方法。它通过分解联合概率 ( p(x_{1:T}) ) 来建模序列 ( x_{1:T} &#x3D; (x_1, x_2, \dots, x_T) )，其中每个词的概率依赖于前面的上下文。</p>
<ul>
<li><strong>联合概率分解</strong>： $p(x_{1:T}) &#x3D; \prod_{t&#x3D;1}^T p(x_t | c_t)$，其中，$( c_t &#x3D; x_{1:t-1} )$ 是当前词  $x_t$ 的上下文（即前面的所有词）。</li>
<li><strong>对数似然</strong>： 为了便于计算，通常使用对数似然：  $\log p(x_{1:T}) &#x3D; \sum_{t&#x3D;1}^T \log p(x_t | c_t)$</li>
<li><strong>特点</strong>：<ul>
<li>自回归模型（如 GPT）是单向的，只能从左到右或从右到左建模。</li>
<li>适合生成任务，因为可以逐词生成文本。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-BERT-的掩码语言建模（Masked-Language-Modeling-MLM）"><a href="#2-BERT-的掩码语言建模（Masked-Language-Modeling-MLM）" class="headerlink" title="2. BERT 的掩码语言建模（Masked Language Modeling, MLM）"></a>2. <strong>BERT 的掩码语言建模（Masked Language Modeling, MLM）</strong></h3><p>BERT 提出了一种双向建模的方法，称为掩码语言建模（MLM）。与自回归模型不同，MLM 通过随机掩码输入序列中的一些词，并让模型基于双向上下文预测这些被掩码的词。</p>
<ul>
<li><strong>输入序列的噪声化</strong>：<ul>
<li>输入序列 ( $x_{1:T}$ ) 被随机掩码，生成一个噪声化的序列 ( $\hat{x}$ )。</li>
<li>被掩码的词记为 ( $\bar{x}$ )。</li>
</ul>
</li>
<li><strong>目标函数</strong>： BERT 的目标是最大化被掩码词的条件概率： $p(\bar{x} | \hat{x}) &#x3D; \prod_{t&#x3D;1}^T p(x_t | c_t)^{m_t}$ </li>
<li>其中：<ul>
<li>( $c_t &#x3D; \hat{x}$ ) 是当前词 ( $x_t$ ) 的双向上下文（包括左右两侧的词）。</li>
<li>( $m_t$ ) 是一个掩码指示器：<ul>
<li>( $m_t$ &#x3D; 1 ) 表示 ( $x_t$ ) 被掩码，需要预测。</li>
<li>( $m_t$ &#x3D; 0 ) 表示 ( $x_t$ ) 未被掩码，不需要预测。</li>
</ul>
</li>
</ul>
</li>
<li><strong>特点</strong>：<ul>
<li>BERT 是双向的，可以同时利用左右两侧的上下文信息。</li>
<li>适合理解任务（如分类、问答等），但不适合直接生成文本。</li>
</ul>
</li>
</ul>
<h3 id="3-对比自回归建模和掩码语言建模"><a href="#3-对比自回归建模和掩码语言建模" class="headerlink" title="3. 对比自回归建模和掩码语言建模"></a>3. <strong>对比自回归建模和掩码语言建模</strong></h3><table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>自回归建模</strong>（如 GPT）</th>
<th><strong>掩码语言建模</strong>（如 BERT）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>方向性</strong></td>
<td>单向（从左到右或从右到左）</td>
<td>双向（同时利用左右上下文）</td>
</tr>
<tr>
<td><strong>目标</strong></td>
<td>预测下一个词</td>
<td>预测被掩码的词</td>
</tr>
<tr>
<td><strong>适合任务</strong></td>
<td>文本生成</td>
<td>文本理解（分类、问答等）</td>
</tr>
<tr>
<td><strong>上下文依赖</strong></td>
<td>仅依赖前面的词</td>
<td>依赖整个序列的双向上下文</td>
</tr>
</tbody></table>
<hr>
<h3 id="4-举例说明"><a href="#4-举例说明" class="headerlink" title="4. 举例说明"></a>4. <strong>举例说明</strong></h3><p>假设有一个序列： $x_{1:5} &#x3D; (\text{The}, \text{cat}, \text{sat}, \text{on}, \text{mat})$</p>
<ul>
<li><strong>自回归建模</strong>：<ul>
<li>分解为： $p(\text{The}) \cdot p(\text{cat} | \text{The}) \cdot p(\text{sat} | \text{The cat}) \cdot \dots$</li>
<li>只能从左到右逐词生成。</li>
</ul>
</li>
<li><strong>掩码语言建模</strong>：<ul>
<li>随机掩码部分词，例如：$\hat{x} &#x3D; (\text{The}, \text{[MASK]}, \text{sat}, \text{[MASK]}, \text{mat})$ ]$</li>
<li>模型需要预测被掩码的词（如 <code>cat</code> 和 <code>on</code>），利用双向上下文。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>这段文字解释了语言建模（LM）和掩码语言建模（MLM）的核心思想：<strong>它们都可以归结为对给定上下文 ( c ) 的条件下，某个词 ( x ) 的条件概率分布建模</strong>。具体来说，这个条件概率通常通过 <strong>softmax 函数</strong> 来建模。以下是详细解释：</p>
<hr>
<h3 id="1-条件概率建模"><a href="#1-条件概率建模" class="headerlink" title="1. 条件概率建模"></a>1. <strong>条件概率建模</strong></h3><p>无论是自回归语言建模（LM）还是掩码语言建模（MLM），它们的核心目标都是建模以下条件概率：<br>[<br>p(x | c)<br>]<br>其中：</p>
<ul>
<li>( x ) 是当前需要预测的词（token）。</li>
<li>( c ) 是上下文（context），即模型在预测 ( x ) 时所依赖的信息。</li>
</ul>
<hr>
<h3 id="2-Softmax-函数"><a href="#2-Softmax-函数" class="headerlink" title="2. Softmax 函数"></a>2. <strong>Softmax 函数</strong></h3><p>条件概率 ( p(x | c) ) 通常通过 softmax 函数来建模：<br>[<br>p(x | c) &#x3D; \frac{\exp(h_c^\top w_x)}{\sum_{x’} \exp(h_c^\top w_{x’})}<br>]<br>其中：</p>
<ul>
<li>( h_c ) 是上下文的嵌入表示（context embedding），它是上下文 ( c ) 的函数。</li>
<li>( w_x ) 是词 ( x ) 的嵌入表示（word embedding），通常通过一个嵌入查找表（embedding lookup table）获得。</li>
<li>( x’ ) 是词汇表中的所有可能词。</li>
</ul>
<hr>
<h3 id="3-上下文嵌入-h-c"><a href="#3-上下文嵌入-h-c" class="headerlink" title="3. 上下文嵌入 ( h_c )"></a>3. <strong>上下文嵌入 ( h_c )</strong></h3><ul>
<li>( h_c ) 是上下文 ( c ) 的向量表示，通常由一个深度神经网络（如 Transformer）生成。</li>
<li>在自回归语言建模中，( c ) 是当前词之前的所有词（例如 ( c &#x3D; x_{1:t-1} )）。</li>
<li>在掩码语言建模中，( c ) 是整个序列的双向上下文（例如 ( c &#x3D; \hat{x} )，即掩码后的序列）。</li>
</ul>
<hr>
<h3 id="4-词嵌入-w-x"><a href="#4-词嵌入-w-x" class="headerlink" title="4. 词嵌入 ( w_x )"></a>4. <strong>词嵌入 ( w_x )</strong></h3><ul>
<li>( w_x ) 是词 ( x ) 的向量表示，通常通过一个嵌入查找表（embedding lookup table）获得。</li>
<li>嵌入查找表是一个矩阵，其中每一行对应词汇表中的一个词。</li>
</ul>
<hr>
<h3 id="5-公式分解"><a href="#5-公式分解" class="headerlink" title="5. 公式分解"></a>5. <strong>公式分解</strong></h3><p>公式：<br>[<br>p(x | c) &#x3D; \frac{\exp(h_c^\top w_x)}{\sum_{x’} \exp(h_c^\top w_{x’})}<br>]<br>可以分解为以下步骤：</p>
<ol>
<li><strong>计算上下文嵌入 ( h_c )</strong>：<ul>
<li>通过深度神经网络（如 Transformer）将上下文 ( c ) 映射为一个向量 ( h_c )。</li>
</ul>
</li>
<li><strong>计算词嵌入 ( w_x )</strong>：<ul>
<li>通过嵌入查找表将词 ( x ) 映射为一个向量 ( w_x )。</li>
</ul>
</li>
<li><strong>计算点积 ( h_c^\top w_x )</strong>：<ul>
<li>计算上下文嵌入 ( h_c ) 和词嵌入 ( w_x ) 的点积，表示当前词 ( x ) 与上下文 ( c ) 的匹配程度。</li>
</ul>
</li>
<li><strong>Softmax 归一化</strong>：<ul>
<li>对所有可能的词 ( x’ ) 计算点积 ( h_c^\top w_{x’} )，并通过 softmax 函数归一化，得到概率分布 ( p(x | c) )。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="6-Transformer-的作用"><a href="#6-Transformer-的作用" class="headerlink" title="6. Transformer 的作用"></a>6. <strong>Transformer 的作用</strong></h3><ul>
<li>Transformer 是一个深度神经网络架构，用于生成上下文嵌入 ( h_c )。</li>
<li>在自回归语言建模中，Transformer 是单向的（如 GPT）。</li>
<li>在掩码语言建模中，Transformer 是双向的（如 BERT）。</li>
</ul>
<hr>
<h3 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. <strong>总结</strong></h3><ul>
<li><strong>核心思想</strong>：无论是 LM 还是 MLM，都可以归结为建模条件概率 ( p(x | c) )。</li>
<li><strong>Softmax 函数</strong>：用于将上下文嵌入和词嵌入的匹配程度转换为概率分布。</li>
<li><strong>上下文嵌入 ( h_c )</strong>：由深度神经网络（如 Transformer）生成，表示上下文信息。</li>
<li><strong>词嵌入 ( w_x )</strong>：通过嵌入查找表获得，表示词的向量表示。</li>
<li><strong>Transformer 的作用</strong>：生成上下文嵌入，是模型的核心组件。</li>
</ul>
<p>通过这种建模方式，语言模型能够有效地捕捉上下文和词之间的关系，从而完成各种自然语言处理任务。</p>
</li>
</ul>
</li>
</ul>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    

                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2025 Yuxi<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>