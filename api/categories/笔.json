{"name":"笔","slug":"笔","count":1,"postlist":[{"title":"关于Transformer","slug":"transformer","date":"2025-02-24T07:29:32.182Z","updated":"2025-02-24T12:43:52.653Z","comments":true,"path":"api/articles/transformer.json","excerpt":"<h2 id=\"总述\"><a href=\"#总述\" class=\"headerlink\" title=\"总述\"></a>总述</h2><h3 id=\"Transformer-全面介绍\"><a href=\"#Transformer-全面介绍\" class=\"headerlink\" title=\"Transformer 全面介绍\"></a>Transformer 全面介绍</h3><hr>\n<h4 id=\"一、背景与意义\"><a href=\"#一、背景与意义\" class=\"headerlink\" title=\"一、背景与意义\"></a>一、背景与意义</h4><p>一个文本变换模型：序列到序列</p>\n<p><strong>Transformer</strong> 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过<strong>自注意力机制（Self-Attention）</strong>完全替代了传统的循环神经网络（RNN）和卷积神经网络（CNN），解决了以下核心问题：  </p>\n<ol>\n<li><strong>长距离依赖</strong>：RNN难以捕捉长序列中的远距离关联。  </li>\n<li><strong>并行计算</strong>：Transformer无需按序列顺序处理数据，可并行计算，大幅提升训练效率。  </li>\n<li><strong>模型泛化</strong>：通过注意力机制动态学习不同位置的权重，适应多样化的上下文关系。</li>\n</ol>","keywords":null,"cover":"https://raw.githubusercontent.com/yuxi1005/yuxi1005.github.io/master/pictures/image-20250224160754290.png","content":null,"text":"总述Transformer 全面介绍一、背景与意义一个文本变换模型：序列到序列Transformer 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过自注意力机制（Self-Attention）完全替代了传统的循环神经","raw":null,"photos":[],"categories":[{"name":"笔","slug":"笔","count":1,"path":"api/categories/笔.json"}],"tags":[{"name":"transformer","slug":"transformer","count":3,"path":"api/tags/transformer.json"},{"name":"读论文","slug":"读论文","count":3,"path":"api/tags/读论文.json"}]}]}