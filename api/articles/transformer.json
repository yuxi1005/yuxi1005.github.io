{"title":"关于Transformer","slug":"transformer","date":"2025-02-24T07:29:32.182Z","updated":"2025-02-24T10:20:35.926Z","comments":true,"path":"api/articles/transformer.json","photos":[],"excerpt":"总述Transformer 全面介绍一、背景与意义一个文本变换模型：序列到序列Transformer 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过自注意力机制（Self-Attention）完全替代了传统的循环神经网络（RNN）和卷积神经网络（CNN），解决了以下核心问题：  长距离依赖：RNN难以捕捉长序列中的远距离关联。  并行计算：Transformer无需按序列顺序处理数据，可并行计算，大幅提升训练效率。  模型泛化：通过注意力机制动态学习不同位置的权重，适应多样化的上下文关系。","covers":["https://raw.githubusercontent.com/yuxi1005/yuxi1005.github.io/master/pictures/image-20250224160754290.png"],"content":"<h2 id=\"总述\"><a href=\"#总述\" class=\"headerlink\" title=\"总述\"></a>总述</h2><h3 id=\"Transformer-全面介绍\"><a href=\"#Transformer-全面介绍\" class=\"headerlink\" title=\"Transformer 全面介绍\"></a>Transformer 全面介绍</h3><hr>\n<h4 id=\"一、背景与意义\"><a href=\"#一、背景与意义\" class=\"headerlink\" title=\"一、背景与意义\"></a>一、背景与意义</h4><p>一个文本变换模型：序列到序列</p>\n<p><strong>Transformer</strong> 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过<strong>自注意力机制（Self-Attention）</strong>完全替代了传统的循环神经网络（RNN）和卷积神经网络（CNN），解决了以下核心问题：  </p>\n<ol>\n<li><strong>长距离依赖</strong>：RNN难以捕捉长序列中的远距离关联。  </li>\n<li><strong>并行计算</strong>：Transformer无需按序列顺序处理数据，可并行计算，大幅提升训练效率。  </li>\n<li><strong>模型泛化</strong>：通过注意力机制动态学习不同位置的权重，适应多样化的上下文关系。</li>\n</ol>\n<span id=\"more\"></span>\n\n<hr>\n<h4 id=\"二、核心架构\"><a href=\"#二、核心架构\" class=\"headerlink\" title=\"二、核心架构\"></a>二、核心架构</h4><p><img src=\"https://raw.githubusercontent.com/yuxi1005/yuxi1005.github.io/master/pictures/image-20250224160754290.png\" alt=\"image-20250224160754290\"></p>\n<p>Transformer由<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>堆叠而成，每层结构独立且参数不共享。以下为关键组件：  </p>\n<h5 id=\"1-编码器（Encoder）\"><a href=\"#1-编码器（Encoder）\" class=\"headerlink\" title=\"1. 编码器（Encoder）\"></a>1. <strong>编码器（Encoder）</strong></h5><ul>\n<li><strong>输入嵌入（Input Embedding）</strong>：将输入词转换为高维向量。  </li>\n<li><strong>位置编码（Positional Encoding）</strong>：为词向量注入位置信息（因Transformer无时序处理能力）。  </li>\n<li><strong>多头自注意力（Multi-Head Self-Attention）</strong>：捕捉词与词之间的全局依赖关系。  </li>\n<li><strong>前馈网络（Feed-Forward Network）</strong>：通过全连接层进行非线性变换。  </li>\n<li><strong>残差连接（Residual Connection）</strong>与<strong>层归一化（Layer Normalization）</strong>：缓解梯度消失，加速收敛。</li>\n</ul>\n<h5 id=\"2-解码器（Decoder）\"><a href=\"#2-解码器（Decoder）\" class=\"headerlink\" title=\"2. 解码器（Decoder）\"></a>2. <strong>解码器（Decoder）</strong></h5><ul>\n<li><strong>掩码多头注意力（Masked Multi-Head Attention）</strong>：防止未来信息泄露（训练时仅关注当前位置之前的词）。  </li>\n<li><strong>编码器-解码器注意力（Encoder-Decoder Attention）</strong>：将编码器的输出作为Key和Value，解码器输入作为Query，实现跨序列对齐。  </li>\n<li>其余结构与编码器类似（前馈网络、残差连接等）。</li>\n</ul>\n<hr>\n<h4 id=\"三、自注意力机制（Self-Attention）\"><a href=\"#三、自注意力机制（Self-Attention）\" class=\"headerlink\" title=\"三、自注意力机制（Self-Attention）\"></a>三、自注意力机制（Self-Attention）</h4><p><strong>自注意力是Transformer的核心</strong>，通过计算词与词之间的关联权重，动态聚合上下文信息。其计算过程如下：  </p>\n<ol>\n<li><p><strong>生成Q、K、V矩阵</strong>：  </p>\n<p>输入向量通过线性变换生成<strong>查询（Query）、键（Key）、值（Value）</strong>矩阵。  </p>\n</li>\n<li><p><strong>计算注意力分数</strong>：  </p>\n<p>通过点积计算词与词之间的相似度，再缩放（防止梯度爆炸）并归一化：<br>$$<br>\\text{Attention}(Q, K, V) &#x3D; \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\<br>{d_k}为Key的维度<br>$$</p>\n</li>\n<li><p><strong>多头注意力（Multi-Head）</strong>：  </p>\n<ul>\n<li>将Q、K、V拆分为多个子空间（如8个“头”），分别计算注意力后拼接结果，增强模型捕捉不同层面信息的能力。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h4 id=\"四、位置编码（Positional-Encoding）\"><a href=\"#四、位置编码（Positional-Encoding）\" class=\"headerlink\" title=\"四、位置编码（Positional Encoding）\"></a>四、位置编码（Positional Encoding）</h4><p>因为transformer是并行计算每个词语的Attention的，所以没法考虑到语序问题。因此引入了位置编码。通过位置编码为输入序列注入位置信息。</p>\n<ul>\n<li><p><strong>正弦与余弦函数</strong>：<br>$$<br>PE_{(pos, 2i)} &#x3D; \\sin\\left(\\frac{pos}{10000^{2i&#x2F;d_{\\text{model}}}}\\right), \\quad<br>PE_{(pos, 2i+1)} &#x3D; \\cos\\left(\\frac{pos}{10000^{2i&#x2F;d_{\\text{model}}}}\\right)<br>$$<br>(pos)：词的位置，(i)：维度索引。  </p>\n<p>可泛化到任意长度的序列，且能通过线性变换捕捉相对位置关系。</p>\n</li>\n</ul>\n<hr>\n<h4 id=\"五、前馈网络（Feed-Forward-Network）\"><a href=\"#五、前馈网络（Feed-Forward-Network）\" class=\"headerlink\" title=\"五、前馈网络（Feed-Forward Network）\"></a>五、前馈网络（Feed-Forward Network）</h4><p>每个注意力层后接一个全连接前馈网络，包含两次线性变换和ReLU激活函数：<br>$$<br>\\text{FFN}(x) &#x3D; \\max(0, xW_1 + b_1)W_2 + b_2<br>$$</p>\n<ul>\n<li>作用：增强模型的非线性表达能力。</li>\n</ul>\n<hr>\n<h4 id=\"六、训练与优化\"><a href=\"#六、训练与优化\" class=\"headerlink\" title=\"六、训练与优化\"></a>六、训练与优化</h4><ol>\n<li><strong>损失函数</strong>：交叉熵损失（如机器翻译任务）。  </li>\n<li><strong>优化器</strong>：Adam优化器，结合学习率预热（Warmup）和衰减策略。  </li>\n<li><strong>正则化</strong>：  <ul>\n<li>Dropout：应用于注意力权重和全连接层。  </li>\n<li>标签平滑（Label Smoothing）：缓解过拟合。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h4 id=\"七、优势与局限性\"><a href=\"#七、优势与局限性\" class=\"headerlink\" title=\"七、优势与局限性\"></a>七、优势与局限性</h4><h5 id=\"优势：\"><a href=\"#优势：\" class=\"headerlink\" title=\"优势：\"></a>优势：</h5><ul>\n<li><strong>并行计算</strong>：显著提升训练速度。  </li>\n<li><strong>全局上下文建模</strong>：自注意力机制可捕捉任意距离的依赖关系。  </li>\n<li><strong>可扩展性</strong>：模型深度和宽度灵活调整（如BERT、GPT等变体）。</li>\n</ul>\n<h5 id=\"局限性：\"><a href=\"#局限性：\" class=\"headerlink\" title=\"局限性：\"></a>局限性：</h5><ul>\n<li><strong>计算复杂度</strong>：序列长度(n)的平方级复杂度（(O(n^2))），难以处理超长序列。  </li>\n<li><strong>位置编码瓶颈</strong>：预设的位置编码可能无法完美适应所有任务。  </li>\n<li><strong>显存消耗</strong>：多头注意力导致参数量较大。</li>\n</ul>\n<hr>\n<h4 id=\"八、重要变体与改进\"><a href=\"#八、重要变体与改进\" class=\"headerlink\" title=\"八、重要变体与改进\"></a>八、重要变体与改进</h4><ol>\n<li><strong>BERT</strong>：仅用编码器，通过掩码语言模型预训练。  </li>\n<li><strong>GPT系列</strong>：仅用解码器，自回归生成文本。  </li>\n<li><strong>Efficient Transformers</strong>：  <ul>\n<li><strong>Longformer</strong>：稀疏注意力机制处理长文本。  </li>\n<li><strong>Linformer</strong>：低秩近似降低计算复杂度。</li>\n</ul>\n</li>\n<li><strong>视觉Transformer（ViT）</strong>：将图像分块输入Transformer。</li>\n</ol>\n<hr>\n<h4 id=\"九、总结\"><a href=\"#九、总结\" class=\"headerlink\" title=\"九、总结\"></a>九、总结</h4><p>Transformer凭借其<strong>自注意力机制</strong>和<strong>并行化设计</strong>，成为深度学习领域的基石模型。它不仅推动了NLP的快速发展，还跨界影响了计算机视觉、语音识别等领域。尽管存在计算复杂度高、长序列处理难等挑战，其灵活性和强大性能使其成为AI模型设计的核心范式。后续的改进模型（如稀疏注意力、模型压缩等）进一步扩展了其应用边界，奠定了其在现代AI中的核心地位。</p>\n<h2 id=\"关于前馈网络层\"><a href=\"#关于前馈网络层\" class=\"headerlink\" title=\"关于前馈网络层\"></a>关于前馈网络层</h2><p>前馈网络层，特别是在前馈神经网络中，扮演着至关重要的角色。前馈神经网络是一种最简单的神经网络，各神经元分层排列，每个神经元只与前一层的神经元相连，接收前一层的输出，并输出给下一层，各层间没有反馈。前馈网络层的作用：</p>\n<ol>\n<li><p><strong>特征提取与转换</strong></p>\n<p>隐藏层（即前馈网络中的中间层）的主要功能是提取输入数据的特征和抽象表示。通过具有多个隐藏层，神经网络可以学习输入数据中越来越复杂和抽象的特征。隐藏层中的每个神经元都接收来自前一层神经元的输入，对其进行处理（如加权、求和、激活等），并将其传递到下一层。这样，隐藏层可以转换输入数据并提取有用的特征，从而使网络能够学习输入和输出之间更复杂和抽象的关系。</p>\n</li>\n<li><p><strong>非线性映射</strong>：</p>\n<p>激活函数（如ReLU、sigmoid、tanh等）在隐藏层中的应用引入了非线性，使得神经网络能够学习和建模输入和输出之间更复杂的非线性关系。</p>\n</li>\n<li><p><strong>信息传递与整合</strong>：</p>\n<p>在前馈神经网络中，信息从输入层流向隐藏层，再流向输出层，而不会循环回馈。这种单向信息传递的方式使得网络结构清晰、易于理解和实现。同时，每一层都接收来自前一层的信息，并对其进行整合和处理，最终生成网络的输出。</p>\n</li>\n</ol>\n","categories":[{"name":"笔","slug":"笔","count":1,"path":"api/categories/笔.json"}],"tags":[{"name":"transformer","slug":"transformer","count":2,"path":"api/tags/transformer.json"},{"name":"读论文","slug":"读论文","count":2,"path":"api/tags/读论文.json"}]}