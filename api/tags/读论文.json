{"name":"读论文","slug":"读论文","count":2,"postlist":[{"title":"Bert论文阅读","slug":"Bert论文阅读","date":"2025-02-24T07:12:00.446Z","updated":"2025-02-24T07:50:18.805Z","comments":true,"path":"api/articles/Bert论文阅读.json","excerpt":"<h3 id=\"总体about-Bert\"><a href=\"#总体about-Bert\" class=\"headerlink\" title=\"总体about Bert\"></a>总体about Bert</h3><h4 id=\"1-概述\"><a href=\"#1-概述\" class=\"headerlink\" title=\"1. 概述\"></a>1. 概述</h4><p>BERT（<strong>Bidirectional Encoder Representations from Transformers</strong>）是谷歌于2018年提出的基于Transformer架构的预训练语言模型。它通过双向上下文理解和大规模无监督预训练，显著提升了自然语言处理（NLP）任务的性能，成为NLP领域的里程碑式模型。</p>","keywords":null,"cover":null,"content":null,"text":"总体about Bert1. 概述BERT（Bidirectional Encoder Representations from Transformers）是谷歌于2018年提出的基于Transformer架构的预训练语言模型。它通过双向上下文理解和大规模无监督预训练，显著提升了","raw":null,"photos":[],"categories":[{"name":"笔记","slug":"笔记","count":1,"path":"api/categories/笔记.json"}],"tags":[{"name":"transformer","slug":"transformer","count":2,"path":"api/tags/transformer.json"},{"name":"读论文","slug":"读论文","count":2,"path":"api/tags/读论文.json"}]},{"title":"关于Transformer","slug":"transformer","date":"2025-02-24T07:29:32.182Z","updated":"2025-02-24T10:20:35.926Z","comments":true,"path":"api/articles/transformer.json","excerpt":"<h2 id=\"总述\"><a href=\"#总述\" class=\"headerlink\" title=\"总述\"></a>总述</h2><h3 id=\"Transformer-全面介绍\"><a href=\"#Transformer-全面介绍\" class=\"headerlink\" title=\"Transformer 全面介绍\"></a>Transformer 全面介绍</h3><hr>\n<h4 id=\"一、背景与意义\"><a href=\"#一、背景与意义\" class=\"headerlink\" title=\"一、背景与意义\"></a>一、背景与意义</h4><p>一个文本变换模型：序列到序列</p>\n<p><strong>Transformer</strong> 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过<strong>自注意力机制（Self-Attention）</strong>完全替代了传统的循环神经网络（RNN）和卷积神经网络（CNN），解决了以下核心问题：  </p>\n<ol>\n<li><strong>长距离依赖</strong>：RNN难以捕捉长序列中的远距离关联。  </li>\n<li><strong>并行计算</strong>：Transformer无需按序列顺序处理数据，可并行计算，大幅提升训练效率。  </li>\n<li><strong>模型泛化</strong>：通过注意力机制动态学习不同位置的权重，适应多样化的上下文关系。</li>\n</ol>","keywords":null,"cover":"https://raw.githubusercontent.com/yuxi1005/yuxi1005.github.io/master/pictures/image-20250224160754290.png","content":null,"text":"总述Transformer 全面介绍一、背景与意义一个文本变换模型：序列到序列Transformer 是谷歌于2017年在论文《Attention Is All You Need》中提出的深度学习模型架构。它通过自注意力机制（Self-Attention）完全替代了传统的循环神经","raw":null,"photos":[],"categories":[{"name":"笔","slug":"笔","count":1,"path":"api/categories/笔.json"}],"tags":[{"name":"transformer","slug":"transformer","count":2,"path":"api/tags/transformer.json"},{"name":"读论文","slug":"读论文","count":2,"path":"api/tags/读论文.json"}]}]}